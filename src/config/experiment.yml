experiment:
  general:
    experiment-name: dp
    dataset: yoochoose
    interaction-data-file: yoochoose-interactions.csv
    # memory monitor for python - if use more memory, memory monitor thread kill the application
    max-memory-mb: 20000

  data-preparation:

    # vertical scaling - -1 for disable
    days: -1

    # horizontal scaling - -1 for disable
    number-of-train-users: 1000000
    number-of-valid-users: 100000

    min-weight: -1
    max-weight: 1

    # data pre-processing for train and validation users
    # raw - no pre-processing {use all interaction types}
    # merge;10 - merge interaction in 10 minutes
    # merge - merge all same interaction
    # end_buy - sequence end with buy (remove last detail_view)
    # end_cart - sequence end with card addition (remove last detail_view)
    # end_buy_merge;10
        #   - sequence end with buy (remove last detail_view) merge same interaction in 10 minutes
    # end_cart_merge;10
    #   - sequence end with cart addition (remove last detail_view) merge same interaction in 10 minutes
    # end_buy_sub - sequence end with item buy and generate all subsequence with min length
    # end_cart_sub - sequence end with cart addition generate all subsequence with min length
    # end_buy_sub_merge - sequence end with buy (remove last detail_view)
        #   - generate all subsequence with min length
    # end_cart_sub_merge - sequence end with card addition (remove last detail_view)
        #   - generate all subsequence with min length
    # end_buy_without_click;10
        #   - sequence end with card addition (remove last detail_view) merge same interaction in 10 minutes
        #   - generate all subsequence with min length
    data-prec: raw

    # for preprocessed and cleaning of the data
    min-ratings-per-user: 3
    max-ratings-per-user: 5000

    train-p: 0.9
    valid-p: 0.05
    test-p: 0.05
    # how many users take in float (percent)
    num-users-take: 1.
    save-to-csv: True

  evaluation:
    # Models to evaluate
    # user-knn, item-knn, reminder, pre-item-knn, lstm, popularity
    # similarity-items-rating - compute similarity rating table for pre-item-knn
    # embeddings-similarity-items-rating - compute similarity on embeddings for evaluation
    algorithms: pre-item-knn

    n-test-users: 50
    top-n: 5

    # last-leave-one-out, leave-one-out
    evaluation-type: last-leave-one-out

    # max out items for one user
    max-in-leave-one-out: 5

    # data pre-processing for train and validation users
    # raw - no pre-processing {use all interaction types}
    # merge;10 - merge interaction in 10 minutes
    # merge - merge all same interaction
    # end_buy - sequence end with buy (remove last detail_view)
    # end_cart - sequence end with card addition (remove last detail_view)
    # end_buy_merge;10
        #   - sequence end with buy (remove last detail_view) merge same interaction in 10 minutes
    # end_cart_merge;10
    #   - sequence end with cart addition (remove last detail_view) merge same interaction in 10 minutes
    # end_buy_sub - sequence end with item buy and generate all subsequence with min length
    # end_cart_sub - sequence end with cart addition generate all subsequence with min length
    # end_buy_sub_merge - sequence end with buy (remove last detail_view)
        #   - generate all subsequence with min length
    # end_cart_sub_merge - sequence end with card addition (remove last detail_view)
        #   - generate all subsequence with min length
    # end_buy_without_click;10
        #   - sequence end with card addition (remove last detail_view) merge same interaction in 10 minutes
        #   - generate all subsequence with min length
    data-prec: raw

  lstm:
    # use fo generate sequences to train and evaluate the lstm network
    min-sequence-length: 3
    max-sequence-length: 40
    # maximum sequences from one user if use generate subsequences
    max-train-seq-one-user: 3

    # context data - weight and timestamp and demonetization on bins
    add-weight: False
    add-timestamps: False
    use-bins: False
    per-seq-bins: False
    # 2^4 - number of bins
    bins-exponent: 2

    # part of loss functions weights
    cosine-weight: 1.
    mse-weight: 0.01
    timestamp-weight: 0.01
    weight-weight: 0.01

    # cache size in MB - how many replaced embeddings keep in memory
    cache-size: 0
    # process use for training NN
    workers: 8

    # True - use LSTM or use GRU
    LSTM: True

    # many-to-one, many-to-many
    model-type: many-to-one
    remove-items-without-embedding: True

    # file structure
    model-postfix: all_data_1
    test-batch-evaluate: True
    # models split by ; - models for evaluation
    models-to-test: e32b32nn1n32m40_all_data_1

    # work as python range split value by ;
    model-load-epoch-range: 1;7
    # evaluation type use generated embedding in network in next step?
    n-rec-cycle: 1
    # replace by real nearest embedding
    nearest-item-cycle: True
    # number of near  embeddings
    k: 5
    # weight by popularity of items
    beta: 0

    # Possibility train existing model with other data
    # Path to model to load and continue training or create new empty model
    model-to-train: None

    embeddings: embeddings_als_dp_f_32_l_1_p_5_B_0_8_M_5_0.emb
    # embedding size
    input-embeddings: 32

    # NN architecture
    layer-size: 32
    n-recurent-layers: 1
    epochs: 10
    batch-size: 32

    # use dropout layer for input - For each step in recurrent layer is use another dropout mask (items in sequence)
    dropout-rate: 0.0
    dropout-all: False

    # dropout in first layer - this drop out use same dropout mask for all step go in recurrent layer
    first-dropout: 0.3
    first-recurrent-dropout: 0.2

    # dropout in next recurrent layers
    dropout: 0.2
    recurrent-dropout: 0.1

    #last dense layer dropout
    dense-dropout: 0.0
    dense-dropout-all: False

    # adam, adagrad, momentum, SGD
    optimizer: adam
    # tanh, relu
    activation: tanh
    recurrent-activation: sigmoid

    #adam : 0.001
    #SGD, momentum, adamgrad : 0.01
    learning-rate: 0.0015
    # step(factor, drop_every), poly(power(0-constant)), None - constant
    ls-scheduler: step
    factor: 0.6
    drop-every: 10
    power: 2

    # Some constrains on weight in network - not use in experiments
    # maxnorm (max value need), nonnegnorm (no value need), minmaxnorm (both mina and max need) or none
    dense-l-constraint: none
    dl-min: -1
    dl-max: 3
    dense-l-bias-constraint: none
    dlb-min: -1
    dlb-max: 3
    lstm-l-constraint: none
    lstm-min: -1
    lstm-max: 3
    lstm-l-bias-constraint: none
    lstmb-min: -1
    lstmb-max: 3
    lstm-l-recurrent-constraint: none
    lstmr-min: -1
    lstmr-max: 3

  user-knn:
    # a little bit change Python range, first is start and second is end,
    # but third is add step if is possible or multiple step if is negative. Same for all model use it in this thesis.
    k: 1 10000 -1.5
    beta: 0.2

  item-knn:
    k: 1 50 -1.3
    beta: 0

  precomputed-similar-items:
    # it is possible use regex for evaluate more precomputed models - fast evaluation
    similar-items-files-regex: sim.*
    k: 1 50 -1.3
    beta: 0

  similarity-items-rating:
    # item knn with results save in file - for example for test some other evaluation test set
    similarity-rating-file: similar_items_ratings_test
    max-n: 10

  # factorization for compute embeddings tables
  factorization:
    parametrizations:
      - factors: 128
        regularization: 1
        iterations: 15
        num-threads: 5
        pruning: 5
        use-bm25: True
        bm25B: 0.8
        bm25M: 5.
        use-gpu: False

      - factors: 64
        regularization: 1
        iterations: 15
        num-threads: 5
        pruning: 5
        use-bm25: True
        bm25B: 0.8
        bm25M: 5.
        use-gpu: False

      - factors: 32
        regularization: 1
        iterations: 15
        num-threads: 5
        pruning: 5
        use-bm25: True
        bm25B: 0.8
        bm25M: 5.
        use-gpu: False




